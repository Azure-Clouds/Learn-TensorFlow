{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Embedding_Course3_W2.ipynb","provenance":[{"file_id":"https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%202%20-%20Lesson%201.ipynb","timestamp":1572335418819}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"P-AhVYeBWgQ3","colab_type":"code","outputId":"e62dd9dd-6753-46a1-b3d2-9b62fafccb0a","executionInfo":{"status":"ok","timestamp":1572485624446,"user_tz":-540,"elapsed":38844,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}},"colab":{"base_uri":"https://localhost:8080/","height":555}},"source":["# NOTE: PLEASE MAKE SURE YOU ARE RUNNING THIS IN A PYTHON3 ENVIRONMENT\n","\n","# import tensorflow as tf\n","# print(tf.__version__)\n","\n","# This is needed for the iterator over the data\n","# But not necessary if you have TF 2.0 installed\n","!pip install tensorflow==2.0.0-beta0\n","\n","\n","# tf.enable_eager_execution()\n","\n","!pip install -q tensorflow-datasets"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0.0-beta0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/19/0d0c7f240db7bcd6b83783b9a89a67f38584d100e23ad5ae93114be92232/tensorflow-2.0.0b0-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n","\u001b[K     |████████████████████████████████| 87.9MB 114kB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (0.2.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (1.12.0)\n","Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 38.6MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (3.10.0)\n","Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n","\u001b[K     |████████████████████████████████| 501kB 30.7MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (0.1.7)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (0.8.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (0.8.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (1.11.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (0.33.6)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (1.1.0)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (1.17.3)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta0) (1.0.8)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta0) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta0) (0.16.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta0) (41.4.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta0) (2.8.0)\n","Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n","  Found existing installation: tensorflow 1.15.0\n","    Uninstalling tensorflow-1.15.0:\n","      Successfully uninstalled tensorflow-1.15.0\n","Successfully installed tb-nightly-1.14.0a20190603 tensorflow-2.0.0b0 tf-estimator-nightly-1.14.0.dev2019060501\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_IoM4VFxWpMR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a3554714-db31-4445-923e-0105e3d71d7c","executionInfo":{"status":"ok","timestamp":1572485995704,"user_tz":-540,"elapsed":944,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}}},"source":["import tensorflow as tf\n","print(\"check tensorflow version: \", tf.__version__)\n","import tensorflow_datasets as tfds\n","imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["check tensorflow version:  2.0.0-beta0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wHQ2Ko0zl7M4","colab_type":"code","outputId":"2a73c6d1-4a22-468b-f790-4c8fe2eb7dd8","executionInfo":{"status":"ok","timestamp":1572487063796,"user_tz":-540,"elapsed":18717,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["import numpy as np\n","\n","train_data, test_data = imdb['train'], imdb['test']\n","\n","training_sentences = []\n","training_labels = []\n","\n","testing_sentences = []\n","testing_labels = []\n","\n","# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n","for s,l in train_data:\n","  training_sentences.append(str(s.numpy()))\n","  training_labels.append(l.numpy())\n","  \n","for s,l in test_data:\n","  testing_sentences.append(str(s.numpy()))\n","  testing_labels.append(l.numpy())\n","  \n","print (\"example training sentences: \", training_sentences[403])\n","print (\"example corresponding training labels: \", training_labels[403]) # 1 is positive review\n","\n","training_labels_final = np.array(training_labels)\n","testing_labels_final = np.array(testing_labels)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["example training sentences:  b'Karl Jr and his dad are now running an army on a remote island. They capture a trio of guys who stumble upon the island. Whom after a while fight back. (well the survivors) This one has non-stop blood, gore and carnage, which would have been good if any of it looked remotely real, or if the production didn\\'t look like it was made with a weeks worth of saved up lunch money (I may be overexxagerating there. it was probably just a couple days worth). The horrendous dubbing didn\\'t bother me as much and I suspect if I had been really drunk, some of it MIGHT have been slightly humorous....maybe. But as it is, at merely 78 minutes the movie still felt way too long by.. Oh I don\\'t know... 78 minutes. Don\\'t waste your time.<br /><br />My Grade: F <br /><br />DVD Extras: Bonus movie: \"Zombie \\'90: Extreme Pestilence\"; and Trailers for other Shock-o-Rama released films'\n","example corresponding training labels:  0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7n15yyMdmoH1","colab_type":"code","outputId":"7729cbc6-ec9f-4f69-c099-e70f73dd6b00","executionInfo":{"status":"ok","timestamp":1572487199063,"user_tz":-540,"elapsed":12189,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}},"colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["vocab_size = 10000 # maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n","embedding_dim = 16 # \n","max_length = 120 # maximum length of of review to be kept\n","trunc_type='post' # trucate review from the end beyond 120 words\n","oov_tok = \"<UNK>\" # unknown  \n","\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n","tokenizer.fit_on_texts(training_sentences) # fit on training sentences \n","\n","word_index = tokenizer.word_index\n","print (\"word index type and length: \", type(word_index), len(word_index))\n","\n","\n","for x in list(word_index)[450:454]:\n","  print (\"example word index key value pair: \", x, word_index[x])\n","\n","sequences = tokenizer.texts_to_sequences(training_sentences)\n","print (\"tokenizer.texts_to_sequences type: \", type(sequences))\n","print (\"length of sequences: \", len(sequences))\n","print (\"an example sequence: \", sequences[450])\n","\n","padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n","# \n","testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n","testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["word index type and length:  <class 'dict'> 86539\n","example word index key value pair:  mr 451\n","example word index key value pair:  overall 452\n","example word index key value pair:  called 453\n","example word index key value pair:  children 454\n","tokenizer.texts_to_sequences type:  <class 'list'>\n","length of sequences:  25000\n","an example sequence:  [1, 17, 2334, 1, 3, 680, 1, 703, 5270, 14, 35, 5659, 2520, 1022, 2, 54, 89, 2284, 394, 5, 2520, 1068, 108, 1, 7456, 600, 33, 2, 274, 7, 24, 226, 126, 363, 2716, 8, 8, 703, 5270, 300, 3704, 6715, 30, 323, 5, 4, 600, 742, 497, 1, 26, 1938, 1226, 7, 2, 1031, 30, 3168, 3704, 48, 61, 682, 85, 76, 2, 3512, 5, 2, 1, 3, 4, 2749, 16, 8704, 2, 1165, 15, 524, 15, 1226, 48, 879, 27, 61, 1225, 3484, 3704, 21, 4960, 1981, 3704, 1, 4, 115, 6, 522, 1226, 562, 10, 21, 24, 4, 159, 115, 10, 21, 164, 2, 1, 9, 148, 113, 5, 4, 600, 777, 1861, 1463, 16, 30, 5, 70, 1165, 8, 8, 10, 253, 41, 192, 2520, 195, 6, 157, 46, 13, 3704, 7, 2, 2778, 268, 482, 160, 600, 48, 6, 192, 273, 3704, 6965, 3, 95, 1113, 4, 1339, 2442, 161, 2520, 48, 193, 2341, 6, 8306, 4, 5459, 8, 8, 163, 1709, 362, 17, 600, 33, 2, 274, 19, 10, 21, 24, 180, 356, 2522, 11, 64, 43, 201, 1, 42, 1, 2, 1305, 122, 12, 197, 679, 802, 703, 5270, 3, 2, 179, 5, 1, 19, 1, 42, 1, 7, 4, 1, 53, 3122, 3, 5068, 19, 12, 7, 134, 4, 88, 394, 47]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9axf0uIXVMhO","outputId":"641a4c61-7f4c-4ac3-9618-e69488c44f99","executionInfo":{"status":"ok","timestamp":1572487228309,"user_tz":-540,"elapsed":1792,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) # reverse the key value pair in word_index dict\n","\n","# compare with the previous cell \n","for y in list(reverse_word_index)[450:454]:\n","  print (\"example word index key value pair: \", y, reverse_word_index[y]) \n","\n","# def decode_review(text):\n","#     return ' '.join([reverse_word_index.get(i, '?') for i in text])\n","\n","# print(decode_review(padded[1]))\n","# print(training_sentences[1])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["example word index key value pair:  451 mr\n","example word index key value pair:  452 overall\n","example word index key value pair:  453 called\n","example word index key value pair:  454 children\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5NEpdhb8AxID","colab_type":"code","outputId":"f66d6510-95fa-4254-bc2f-df6193a4065e","executionInfo":{"status":"ok","timestamp":1572487233167,"user_tz":-540,"elapsed":953,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}},"colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(6, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 120, 16)           160000    \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1920)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 6)                 11526     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 7         \n","=================================================================\n","Total params: 171,533\n","Trainable params: 171,533\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V5LLrXC-uNX6","colab_type":"code","outputId":"b22e6b24-f6cb-4bea-a962-46be05130eb4","executionInfo":{"status":"ok","timestamp":1572487488505,"user_tz":-540,"elapsed":48882,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}},"colab":{"base_uri":"https://localhost:8080/","height":523}},"source":["num_epochs = 10\n","model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/10\n","25000/25000 [==============================] - 5s 200us/sample - loss: 0.4920 - accuracy: 0.7485 - val_loss: 0.3846 - val_accuracy: 0.8269\n","Epoch 2/10\n","25000/25000 [==============================] - 5s 189us/sample - loss: 0.2412 - accuracy: 0.9056 - val_loss: 0.3768 - val_accuracy: 0.8345\n","Epoch 3/10\n","25000/25000 [==============================] - 5s 187us/sample - loss: 0.0957 - accuracy: 0.9757 - val_loss: 0.4435 - val_accuracy: 0.8294\n","Epoch 4/10\n","25000/25000 [==============================] - 5s 188us/sample - loss: 0.0247 - accuracy: 0.9969 - val_loss: 0.5189 - val_accuracy: 0.8261\n","Epoch 5/10\n","25000/25000 [==============================] - 5s 188us/sample - loss: 0.0060 - accuracy: 0.9995 - val_loss: 0.5855 - val_accuracy: 0.8266\n","Epoch 6/10\n","25000/25000 [==============================] - 5s 189us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 0.8268\n","Epoch 7/10\n","25000/25000 [==============================] - 5s 187us/sample - loss: 8.9690e-04 - accuracy: 1.0000 - val_loss: 0.6747 - val_accuracy: 0.8286\n","Epoch 8/10\n","25000/25000 [==============================] - 5s 188us/sample - loss: 4.8627e-04 - accuracy: 1.0000 - val_loss: 0.7166 - val_accuracy: 0.8275\n","Epoch 9/10\n","25000/25000 [==============================] - 5s 188us/sample - loss: 2.7565e-04 - accuracy: 1.0000 - val_loss: 0.7501 - val_accuracy: 0.8278\n","Epoch 10/10\n","25000/25000 [==============================] - 5s 189us/sample - loss: 1.6326e-04 - accuracy: 1.0000 - val_loss: 0.7859 - val_accuracy: 0.8275\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f0c51b6ccc0>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"yAmjJqEyCOF_","colab_type":"code","outputId":"91a01823-74c1-48ef-ad7a-43f55ba0e474","executionInfo":{"status":"ok","timestamp":1572487497485,"user_tz":-540,"elapsed":1190,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["e = model.layers[0] # 0th layer or the 1st layer is indeed the embedding layer\n","weights = e.get_weights()[0] # get weights from the embeddings layer\n","print(weights.shape) # shape: (vocab_size, embedding_dim) # we are considering a 16 dimensional space "],"execution_count":9,"outputs":[{"output_type":"stream","text":["(10000, 16)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qp-uWtjXldfX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"464588b7-3bc6-4406-ef99-1c44118ca53a","executionInfo":{"status":"ok","timestamp":1572488029085,"user_tz":-540,"elapsed":21591,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jmB0Uxk0ycP6","colab_type":"code","colab":{}},"source":["import io\n","\n","out_v = io.open('/content/gdrive/My Drive/Colab Notebooks/NLP_Week2/vecs.tsv', 'w', encoding='utf-8')\n","out_m = io.open('/content/gdrive/My Drive/Colab Notebooks/NLP_Week2/meta.tsv', 'w', encoding='utf-8')\n","for word_num in range(1, vocab_size):\n","  word = reverse_word_index[word_num]\n","  embeddings = weights[word_num]\n","  out_m.write(word + \"\\n\")\n","  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n","out_v.close()\n","out_m.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dm4nFwImnl67","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDeqpOCVydtq","colab_type":"code","colab":{}},"source":["\n","## !!!!! Not needed !!!! \n","# try:\n","#   from google.colab import files\n","# except ImportError:\n","#   pass\n","# else:\n","#   files.download('vecs.tsv')\n","#   files.download('meta.tsv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRxoxc2apscY","colab_type":"code","outputId":"3626e62a-fe6b-4fe5-fe77-d7af4c9e76e6","executionInfo":{"status":"ok","timestamp":1572487762387,"user_tz":-540,"elapsed":974,"user":{"displayName":"Swap vi","photoUrl":"","userId":"01936573407644251994"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["sentence = [\"The movie was awful. Don't waste your money on it.\"]\n","sequence_test = tokenizer.texts_to_sequences(sentence)\n","print(sequence_test)\n","\n","padded_test = pad_sequences(sequence_test,maxlen=max_length, truncating=trunc_type)\n","\n","ypr = model.predict(padded_test)  \n","print (\"prediction label: \", ypr)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[[2, 18, 14, 380, 174, 446, 131, 277, 22, 10]]\n","prediction label:  [[6.153002e-05]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wvl_Mp3OfblH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}